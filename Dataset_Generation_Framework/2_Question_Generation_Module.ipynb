{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named-Entity QG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Passing\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Passing\", category=UserWarning)\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class QGPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        qg_model_name: str,\n",
    "        min_para_token_num: int,\n",
    "        min_anssent_token_num: int,\n",
    "        ans_ent_type_exclude_list: list\n",
    "    ):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(qg_model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(qg_model_name)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "        self.min_para_token_num = min_para_token_num\n",
    "        self.min_anssent_token_num = min_anssent_token_num\n",
    "        self.ans_ent_type_exclude_list = ans_ent_type_exclude_list\n",
    "        \n",
    "    def __call__(self, inputs: str):\n",
    "        nlp_inputs = nlp(\" \".join(inputs.split()))\n",
    "        output=[]\n",
    "        if len(nlp_inputs) < self.min_para_token_num:\n",
    "            return output\n",
    "        context = nlp_inputs.text\n",
    "        sents, sents_pos, answers_pos = self._extract_answers_by_NER(nlp_inputs,context)\n",
    "        flat_answers_pos = list(itertools.chain(*answers_pos))\n",
    "        if len(flat_answers_pos) == 0:\n",
    "            return output\n",
    "        qg_examples = self._prepare_inputs_for_qg_from_answers_hl(context, sents, sents_pos, answers_pos)\n",
    "        qg_inputs = [example['source_text'] for example in qg_examples]\n",
    "        questions = self._generate_questions(qg_inputs)\n",
    "        que_list=[]\n",
    "        for example, que in zip(qg_examples, questions):\n",
    "            if que in que_list:\n",
    "                continue\n",
    "            que_list.append(que)\n",
    "            output.append({'question':que, 'answer':example['answer'], 'ans_pos':example['answer_pos'], 'ans-sent_pos':example['answer-sent_pos']})\n",
    "        return output\n",
    "    \n",
    "    def _extract_answers_by_NER(self, nlp_context, context):\n",
    "        sents, sents_pos=[], []\n",
    "        answers_pos=[]\n",
    "        for sent in nlp_context.sents:\n",
    "            sents.append(sent.text)\n",
    "            sents_pos.append([sent.start_char, sent.end_char])\n",
    "            sent_ans_pos=[]\n",
    "            if len(sent)< self.min_anssent_token_num:\n",
    "                answers_pos.append(sent_ans_pos)\n",
    "            else:\n",
    "                for ent in list(sent.ents):\n",
    "                    if context[ent.start_char:ent.end_char]==ent.text:\n",
    "                        if ent.label_ not in self.ans_ent_type_exclude_list:\n",
    "                            sent_ans_pos.append([ent.start_char,ent.end_char])\n",
    "                answers_pos.append(sent_ans_pos)\n",
    "        return sents, sents_pos, answers_pos\n",
    "    \n",
    "    def _prepare_inputs_for_qg_from_answers_hl(self, context, sents, sents_pos, answers_pos):\n",
    "        inputs = []\n",
    "        for i, answer_pos in enumerate(answers_pos):\n",
    "            if len(answer_pos) == 0: \n",
    "                continue\n",
    "            for pos_tuple in answer_pos:\n",
    "                sents_copy = sents[:]\n",
    "                sent = sents_copy[i] #correct sent\n",
    "                answer_check = context[pos_tuple[0]:pos_tuple[1]]\n",
    "                ans_start_idx = pos_tuple[0]-sents_pos[i][0]\n",
    "                ans_end_idx = pos_tuple[1]-sents_pos[i][0]\n",
    "                answer_text=sent[ans_start_idx:ans_end_idx]\n",
    "                assert answer_text==answer_check\n",
    "                try:\n",
    "                    sent = f\"{sent[:ans_start_idx]} <hl> {answer_text} <hl> {sent[ans_end_idx: ]}\"\n",
    "                    sents_copy[i] = sent\n",
    "                    source_text = \" \".join(sents_copy)\n",
    "                    source_text = f\"generate question: {source_text}\" \n",
    "                    source_text = source_text + \" </s>\"\n",
    "                    inputs.append({\"answer\": answer_text, \"answer_pos\": pos_tuple, \"answer-sent_pos\": sents_pos[i], \"source_text\": source_text})\n",
    "                except:\n",
    "                    continue\n",
    "        return inputs\n",
    "\n",
    "    def _tokenize(self,inputs):\n",
    "        inputs = self.tokenizer(inputs,max_length=512,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "        return inputs\n",
    "    \n",
    "    def _generate_questions(self, inputs):\n",
    "        inputs = self._tokenize(inputs)\n",
    "        outs = self.model.generate(\n",
    "            input_ids=inputs['input_ids'].to(self.device), \n",
    "            attention_mask=inputs['attention_mask'].to(self.device), \n",
    "            max_length=32,\n",
    "            num_beams=4,\n",
    "        )\n",
    "        questions = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "        return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=pickle.load(open(\"data/examples.pickle\", \"rb\"))\n",
    "\n",
    "qg_model_name=\"valhalla/t5-base-qg-hl\"\n",
    "min_para_token_num = 30\n",
    "min_anssent_token_num = 10\n",
    "qg_model = QGPipeline(qg_model_name, min_para_token_num, min_anssent_token_num, ans_ent_type_exclude_list=[])\n",
    "# The QG model in our work is a modified version based on \"valhalla/t5-base-qg-hl\", and the QGPipeline class here is based on \"valhalla/t5-base-qg-hl\".\n",
    "# Other exisiting awesome qg models are also available in huggingface, for example, \"mrm8488/t5-base-finetuned-question-generation-ap\".\n",
    "# The paragraphs that have less than min_para_token_num tokens are eliminated.\n",
    "# The answers whose corresponding sentences have less than 10 tokens are eliminated.\n",
    "# The answers whose corresponding NER types in ans_ent_type_exclude_list([\"PERSON\", \"ORG\", ...], but not used in our work) are eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate 90 questions.\n"
     ]
    }
   ],
   "source": [
    "total_results = []\n",
    "for example in examples:\n",
    "    para_id, para_pub, para_text = example\n",
    "    results = qg_model(para_text)\n",
    "    for r in results:\n",
    "        r[\"para_id\"] = para_id\n",
    "    total_results.extend(results)\n",
    "    \n",
    "print(f\"Generate {len(total_results)} questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>ans_pos</th>\n",
       "      <th>ans-sent_pos</th>\n",
       "      <th>para_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which country's hard-line Revolutionary Guard ...</td>\n",
       "      <td>Iran</td>\n",
       "      <td>[0, 4]</td>\n",
       "      <td>[0, 249]</td>\n",
       "      <td>1650014_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What group said the death sentence against Sal...</td>\n",
       "      <td>Revolutionary Guard</td>\n",
       "      <td>[17, 36]</td>\n",
       "      <td>[0, 249]</td>\n",
       "      <td>1650014_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On what day did Iran's Revolutionary Guard say...</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>[45, 53]</td>\n",
       "      <td>[0, 249]</td>\n",
       "      <td>1650014_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who was sentenced to death for blasphemy again...</td>\n",
       "      <td>Salman Rushdie</td>\n",
       "      <td>[86, 100]</td>\n",
       "      <td>[0, 249]</td>\n",
       "      <td>1650014_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who condemned Salman Rushdie?</td>\n",
       "      <td>Ayatollah Ruhollah Khomeini</td>\n",
       "      <td>[125, 152]</td>\n",
       "      <td>[0, 249]</td>\n",
       "      <td>1650014_0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Which country's hard-line Revolutionary Guard ...   \n",
       "1  What group said the death sentence against Sal...   \n",
       "2  On what day did Iran's Revolutionary Guard say...   \n",
       "3  Who was sentenced to death for blasphemy again...   \n",
       "4                      Who condemned Salman Rushdie?   \n",
       "\n",
       "                        answer     ans_pos ans-sent_pos    para_id  \n",
       "0                         Iran      [0, 4]     [0, 249]  1650014_0  \n",
       "1          Revolutionary Guard    [17, 36]     [0, 249]  1650014_0  \n",
       "2                     Saturday    [45, 53]     [0, 249]  1650014_0  \n",
       "3               Salman Rushdie   [86, 100]     [0, 249]  1650014_0  \n",
       "4  Ayatollah Ruhollah Khomeini  [125, 152]     [0, 249]  1650014_0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_results_df = pd.DataFrame(total_results)\n",
    "total_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the raw QG results\n",
    "total_results_df.to_feather(\"data/raw_results_After_2ndModule.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
