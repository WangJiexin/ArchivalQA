{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import neuralcoref\n",
    "#packages used in the step of removing questions with unclear pronouns\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "from python_heideltime.python_heideltime import Heideltime\n",
    "#packages used in the step of transforming temporal information\n",
    "#Install the following package in the current directory\n",
    "#https://github.com/PhilipEHausner/python_heideltime\n",
    "#Note that as there are some problems with the current SUTime version (which was actually used in the paper),\n",
    "#we simply replace with heideltime for temporal transformation (which may result in different results).\n",
    "heideltime_parser = Heideltime()\n",
    "heideltime_parser.set_document_type('NEWS')\n",
    "temp_extract_re = re.compile(r'type=\"DATE\" value=(.*?)>(.*?)(</TIMEX3>)')\n",
    "temp_trans_re = re.compile(r'<TIMEX3(.*?)</TIMEX3>')\n",
    "temp_day_form=\"\\d{4}-\\d{2}-\\d{2}\"\n",
    "temp_month_form=\"\\d{4}-\\d{2}\"\n",
    "temp_year_form=\"\\d{4}\"\n",
    "idx2month=dict()\n",
    "month_list=[\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\\\n",
    "\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "for i,month_v in enumerate(month_list):\n",
    "    m_idx=str(i+1).zfill(2)\n",
    "    idx2month[m_idx]=month_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     15,
     32,
     37,
     42,
     47,
     57,
     67,
     74,
     85,
     90
    ]
   },
   "outputs": [],
   "source": [
    "class Syntactic_Temporal_Processing:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_ent_num: int, \n",
    "        max_ent_num: int, \n",
    "        min_token_num: int, \n",
    "        max_token_num: int, \n",
    "        paraid_pub_dict: dict,\n",
    "    ):\n",
    "        self.min_ent_num = min_ent_num\n",
    "        self.max_ent_num = max_ent_num\n",
    "        self.min_token_num = min_token_num\n",
    "        self.max_token_num = max_token_num\n",
    "        self.paraid_pub_dict = paraid_pub_dict\n",
    "        \n",
    "    def __call__(self, raw_results_df):\n",
    "        self.results_df = raw_results_df\n",
    "        self.remove_que_without_qmark()\n",
    "        self.remove_que_ansinquestion()\n",
    "        self.remove_que_duplicate()\n",
    "        self.add_nlp_column()\n",
    "        self.remove_que_entissue()\n",
    "        self.remove_que_tokissue()\n",
    "        self.remove_que_pronissue()\n",
    "        self.add_transformation_columns()\n",
    "        self.transform_que_temp()\n",
    "        self.transform_ans_temp()\n",
    "        return self.results_df\n",
    "    \n",
    "    def add_nlp_column(self):\n",
    "        self.results_df['que_nlp'] = self.results_df['question'].apply(lambda x: nlp(x))\n",
    "        \n",
    "    def remove_que_without_qmark(self):\n",
    "        #1. Remove questions that do not end with a question mark.\n",
    "        self.results_df = self.results_df[self.results_df['question'].str.endswith('?')]\n",
    "        self.results_df = self.results_df.reset_index(drop=True)\n",
    "        \n",
    "    def remove_que_ansinquestion(self):\n",
    "        #2. Remove questions whose answers are explicitly indicated inside the questionsâ€™ content.\n",
    "        self.results_df = self.results_df[self.results_df.apply(lambda x: x[\"answer\"].lower() not in x[\"question\"].lower(), axis=1)]\n",
    "        self.results_df = self.results_df.reset_index(drop=True)\n",
    "        \n",
    "    def remove_que_duplicate(self):\n",
    "        #3. Remove duplicate questions.\n",
    "        self.results_df = self.results_df[~self.results_df.duplicated('question',keep=False)]\n",
    "        self.results_df = self.results_df.reset_index(drop=True)\n",
    "        \n",
    "    def remove_que_entissue(self):\n",
    "        #4. Remove questions that have too few or too many named entities.\n",
    "        removed_index = []\n",
    "        for row_i,row in self.results_df.iterrows():\n",
    "            if min_ent_num<=len(row['que_nlp'].ents)<=max_ent_num:\n",
    "                continue\n",
    "            else:\n",
    "                removed_index.append(row_i)\n",
    "        self.results_df = self.results_df.drop(removed_index).reset_index(drop=True)\n",
    "    \n",
    "    def remove_que_tokissue(self):\n",
    "        #5. Remove questions that are too short or too long.\n",
    "        removed_index = []\n",
    "        for row_i,row in self.results_df.iterrows():\n",
    "            if min_token_num<=len(row['que_nlp'])<=max_token_num:\n",
    "                continue\n",
    "            else:\n",
    "                removed_index.append(row_i)\n",
    "        self.results_df = self.results_df.drop(removed_index).reset_index(drop=True)\n",
    "    \n",
    "    def remove_que_pronissue(self):\n",
    "        #6. Remove questions with unclear pronouns\n",
    "        question_set = set(self.results_df[\"question\"])\n",
    "        Pron_Processing = Pronouns_Question_Processing(question_set)\n",
    "        good_pron_question, bad_pron_question = Pron_Processing()\n",
    "        self.results_df = self.results_df[~self.results_df['question'].isin(bad_pron_question)].reset_index(drop=True)\n",
    "    \n",
    "    def add_transformation_columns(self):\n",
    "        self.results_df[\"org_question\"] = self.results_df[\"question\"]\n",
    "        self.results_df[\"trans_question\"] = self.results_df[\"question\"]\n",
    "        self.results_df = self.results_df.drop(['question'], axis=1)\n",
    "        self.results_df[\"org_answer\"] = self.results_df[\"answer\"]\n",
    "        self.results_df[\"trans_answer\"] = self.results_df[\"answer\"]\n",
    "        self.results_df = self.results_df.drop(['answer'], axis=1)\n",
    "        self.results_df[\"trans_que\"] = \"0\"\n",
    "        self.results_df[\"trans_ans\"] = \"0\"\n",
    "        self.results_df = self.results_df[[\"org_question\",\"trans_question\",\"org_answer\",\"trans_answer\",\"ans_pos\",\"ans-sent_pos\",\"para_id\",\"trans_que\",\"trans_ans\"]]\n",
    "    \n",
    "    def transform_que_temp(self):\n",
    "        #7. Transform relative temporal information in questions to absolute temporal information.\n",
    "        Temp_Processing = TempInfor_Transformation_Processing(self.results_df, self.paraid_pub_dict, \"question\")\n",
    "        self.results_df = Temp_Processing()\n",
    "    \n",
    "    def transform_ans_temp(self):\n",
    "        #8. Transform relative temporal information of the answers of generated questions to absolute temporal information.\n",
    "        Temp_Processing = TempInfor_Transformation_Processing(self.results_df, self.paraid_pub_dict, \"answer\")\n",
    "        self.results_df = Temp_Processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     16,
     56,
     68,
     125,
     154,
     192,
     239,
     255,
     290,
     308,
     319,
     349,
     387,
     434,
     451,
     486
    ]
   },
   "outputs": [],
   "source": [
    "class Pronouns_Question_Processing:\n",
    "    def __init__(self, question_set):\n",
    "        self.question_set = question_set\n",
    "        self.pron_tag_set=set([\"PRP\",\"PRP$\"])\n",
    "        self.results_df = question_set\n",
    "        self.myyou_words_set=set(['i','we','me','us','mine','ours','my','our','myself','ourselves',\\\n",
    "                            'you','yours','your','yourself','yourselves'])\n",
    "        \n",
    "    def __call__(self):\n",
    "        self.pronouns_questions_recognization()\n",
    "        self.common_question_processing()\n",
    "        self.pronque_nocoref_processing()\n",
    "        good_question_set = self.good_common_question|self.good_nocoref_question\n",
    "        bad_question_set = self.bad_common_question|self.bad_nocoref_question\n",
    "        return [good_question_set, bad_question_set]\n",
    "    \n",
    "    def pronouns_questions_recognization(self):\n",
    "        pron_tag_list=[\"PRP\",\"PRP$\"]\n",
    "        self.pron_que_nlp_dict=dict()\n",
    "        self.que_coref_dict=dict()\n",
    "        for que in self.question_set:\n",
    "            que_nlp = nlp(que)\n",
    "            #1. Identify questions with pronouns\n",
    "            pron_flag = False\n",
    "            for token in que_nlp:\n",
    "                if token.tag_ in pron_tag_list:\n",
    "                    self.pron_que_nlp_dict[que] = que_nlp\n",
    "                    pron_flag = True\n",
    "                    break\n",
    "            if not pron_flag:\n",
    "                continue\n",
    "            #2. Collect coreference information of questions with pronouns\n",
    "            que_coref=que_nlp._.coref_clusters\n",
    "            if len(que_coref)==0:\n",
    "                continue\n",
    "            que_coref_list=[]\n",
    "            for core_infor in que_coref:\n",
    "                main_ref=core_infor.main\n",
    "                main_begin_idx=main_ref[0].idx\n",
    "                main_end_idx=main_ref[-1].idx+len(main_ref[-1].text)\n",
    "                assert main_ref.text==que[main_begin_idx:main_end_idx]\n",
    "\n",
    "                cluster_ref_list=core_infor.mentions\n",
    "                cluster_ref_postuple_list=[]\n",
    "                for ref_ele in cluster_ref_list:\n",
    "                    ref_ele_begin_idx=ref_ele[0].idx\n",
    "                    ref_ele_end_idx=ref_ele[-1].idx+len(ref_ele[-1].text)\n",
    "                    assert ref_ele.text==que[ref_ele_begin_idx:ref_ele_end_idx]\n",
    "                    cluster_ref_postuple_list.append([ref_ele_begin_idx,ref_ele_end_idx])\n",
    "                que_coref_list.append([(main_begin_idx,main_end_idx),cluster_ref_postuple_list])\n",
    "            self.que_coref_dict[que]=que_coref_list\n",
    "        #questions with pronouns and coreference information\n",
    "        self.common_que_set=set(self.que_coref_dict.keys())\n",
    "        #questions with pronouns only\n",
    "        self.pronque_nocoref_set=set(self.pron_que_nlp_dict.keys())-self.common_que_set\n",
    "        \n",
    "    def common_question_processing(self):\n",
    "        self.common_candidate_question = self.common_que_set\n",
    "        self.good_common_question = set()\n",
    "        self.bad_common_question = set()\n",
    "        self.common_question_processing_step1()\n",
    "        self.common_question_processing_step2()\n",
    "        self.common_question_processing_step3()\n",
    "        self.common_question_processing_step4()\n",
    "        self.common_question_processing_step5()\n",
    "        self.common_question_processing_step6()\n",
    "        self.common_question_processing_step7()\n",
    "\n",
    "    def common_question_processing_step1(self):\n",
    "        for question in self.common_candidate_question:\n",
    "            coref_infor=self.que_coref_dict[question]\n",
    "            coref_pos_list=[]\n",
    "            coref_postext_list=[]\n",
    "            notmain_coref_pos_set=set()\n",
    "            main_coref_pos_set=set()\n",
    "            for pos_tuple in coref_infor:\n",
    "                mainpos_tuple_beg=pos_tuple[0][0]\n",
    "                mainpos_tuple_end=pos_tuple[0][1]\n",
    "                coref_pos_list.append([f\"{mainpos_tuple_beg}-{mainpos_tuple_end}\"])\n",
    "                main_coref_pos_set.add(f\"{mainpos_tuple_beg}_{mainpos_tuple_end}\")\n",
    "                coref_postext_list.append([question[mainpos_tuple_beg:mainpos_tuple_end].lower()])\n",
    "                for pos in pos_tuple[1]:\n",
    "                    pos_beg=pos[0]\n",
    "                    pos_end=pos[1]\n",
    "                    if f\"{pos_beg}-{pos_end}\" in coref_pos_list[-1]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        coref_pos_list[-1].append(f\"{pos_beg}_{pos_end}\")\n",
    "                        notmain_coref_pos_set.add(f\"{pos_beg}_{pos_end}\")\n",
    "                        coref_postext_list[-1].append(question[pos_beg:pos_end].lower())\n",
    "            nlp_infor=self.pron_que_nlp_dict[question]\n",
    "            token_pronidx_set=set()\n",
    "            token_prontext_list=[]\n",
    "            for token in nlp_infor:\n",
    "                if token.tag_ in self.pron_tag_set:\n",
    "                    token_pronidx_infor=f\"{token.idx}_{token.idx+len(token.text)}\"\n",
    "                    token_pronidx_set.add(token_pronidx_infor)\n",
    "                    token_prontext_list.append(question[token.idx:token.idx+len(token.text)].lower())\n",
    "\n",
    "            #Remove myyou_words cases\n",
    "            token_prontext_lower_set=set(prontext.lower() for prontext in token_prontext_list)\n",
    "            if token_prontext_lower_set.intersection(self.myyou_words_set):\n",
    "                continue\n",
    "\n",
    "            if len(token_pronidx_set-notmain_coref_pos_set)==0:\n",
    "                self.good_common_question.add(question)\n",
    "            else:\n",
    "                check_remained_pronidx_set=token_pronidx_set-notmain_coref_pos_set\n",
    "                not_common_coref_pos_set=notmain_coref_pos_set-token_pronidx_set\n",
    "                correct_remained_pronidx_num=0\n",
    "                for pronidx_tuple in check_remained_pronidx_set:\n",
    "                    pronidx_beg,pronidx_end=list(map(int, pronidx_tuple.split(\"_\")))\n",
    "                    for corefidx_tuple in not_common_coref_pos_set:\n",
    "                        corefidx_beg,corefidx_end=list(map(int, corefidx_tuple.split(\"_\")))\n",
    "                        if corefidx_beg<=pronidx_beg<=pronidx_end<=corefidx_end:\n",
    "                            correct_remained_pronidx_num+=1\n",
    "                if correct_remained_pronidx_num==len(check_remained_pronidx_set):\n",
    "                    self.good_common_question.add(question)\n",
    "                    \"\"\"\n",
    "                    print(question)            #Who does Greece want to protect outside of their country?\n",
    "                    print(coref_postext_list)  #[['greece', 'their country']]\n",
    "                    print(token_prontext_list) #['their']\n",
    "                    \"\"\"\n",
    "        self.common_candidate_question=self.common_candidate_question-self.good_common_question-self.bad_common_question\n",
    "\n",
    "    def common_question_processing_step2(self):\n",
    "        who_re = re.compile(r\"(Who|Whose|Whom) (.*?)(say|says|saying|said)(.*)\")\n",
    "        for question in self.common_candidate_question:\n",
    "            re_result=who_re.match(question)\n",
    "            if re_result:\n",
    "                between_text=who_re.search(question).group(2)\n",
    "                if len(between_text.strip())==0:\n",
    "                    self.good_common_question.add(question)\n",
    "                else:\n",
    "                    between_text_pos_beg,between_text_pos_end=who_re.match(question).span(2)\n",
    "                    nlp_infor=self.pron_que_nlp_dict[question]\n",
    "                    myyoutoken_pronidx_set=set()\n",
    "                    myyoutoken_prontext_list=[]\n",
    "                    for token in nlp_infor:\n",
    "                        if token.tag_ in self.pron_tag_set:\n",
    "                            if question[token.idx:token.idx+len(token.text)] in self.myyou_words_set:\n",
    "                                token_pronidx_infor=f\"{token.idx}_{token.idx+len(token.text)}\"\n",
    "                                myyoutoken_pronidx_set.add(token_pronidx_infor)\n",
    "                                myyoutoken_prontext_list.append(question[token.idx:token.idx+len(token.text)].lower())\n",
    "                    add_question=True\n",
    "                    for pronidx_tuple in myyoutoken_pronidx_set:\n",
    "                        pronidx_beg,pronidx_end=list(map(int, pronidx_tuple.split(\"_\")))\n",
    "                        if between_text_pos_beg<=pronidx_beg<=pronidx_end<=between_text_pos_end:\n",
    "                            add_question=False\n",
    "                    if add_question:\n",
    "                        self.good_common_question.add(question)\n",
    "\n",
    "        self.common_candidate_question=self.common_candidate_question-self.good_common_question-self.bad_common_question\n",
    "        \n",
    "    def common_question_processing_step3(self):\n",
    "        who_words_set = set([\"who\",\"whose\",\"whom\"])\n",
    "        he_words_set=set(['he', 'him', 'himself', 'his'])\n",
    "        she_words_set=set(['she', 'her','hers', 'herself'])\n",
    "        they_words_set=set(['they','them','theirs','their','themselves'])\n",
    "\n",
    "        for question in self.common_candidate_question:\n",
    "            nlp_infor=self.pron_que_nlp_dict[question]\n",
    "            token_text_set=set([token.text.lower() for token in nlp_infor])\n",
    "\n",
    "            token_pronidx_set=set()\n",
    "            token_prontext_set=set()\n",
    "            for token in nlp_infor:\n",
    "                if token.tag_ in self.pron_tag_set:\n",
    "                    token_pronidx_infor=f\"{token.idx}_{token.idx+len(token.text)}\"\n",
    "                    token_pronidx_set.add(token_pronidx_infor)\n",
    "                    token_prontext_set.add(question[token.idx:token.idx+len(token.text)].lower())\n",
    "\n",
    "            if token_text_set.intersection(who_words_set):\n",
    "                if token_prontext_set.intersection(self.myyou_words_set):\n",
    "                    continue\n",
    "                else:\n",
    "                    he_in_tag=False\n",
    "                    she_in_tag=False\n",
    "                    they_in_tag=False\n",
    "                    if token_prontext_set.intersection(he_words_set):\n",
    "                        he_in_tag=True\n",
    "                    if token_prontext_set.intersection(she_words_set):\n",
    "                        she_in_tag=True\n",
    "                    if token_prontext_set.intersection(they_words_set):\n",
    "                        they_in_tag=True\n",
    "                    if int(he_in_tag)+int(she_in_tag)+int(they_in_tag)>1:\n",
    "                        self.bad_common_question.add(question)\n",
    "                        continue\n",
    "                    self.good_common_question.add(question)\n",
    "\n",
    "        self.common_candidate_question=self.common_candidate_question-self.good_common_question-self.bad_common_question\n",
    "        \n",
    "    def common_question_processing_step4(self):\n",
    "        who_re2 = re.compile(r\"(.+?)(\\\"(.+)\\\"|''(.+)'')(.*)\")\n",
    "        who_words_set = set([\"who\",\"whose\",\"whom\"])\n",
    "        heshethey_words_set=set(['he', 'she', 'they', 'her', 'him', 'them', 'himself',\\\n",
    "                             'hers', 'theirs', 'his', 'their', 'herself', 'themselves'])\n",
    "        select_ner_set=set([\"PERSON\",\"NORP\",\"FAC\",\"ORG\",\"GPE\"])\n",
    "\n",
    "        for question in self.common_candidate_question:\n",
    "            re_result=who_re2.match(question)\n",
    "            if re_result:\n",
    "                nlp_infor=self.pron_que_nlp_dict[question]\n",
    "                token_text_set=set([token.text.lower() for token in nlp_infor])\n",
    "                between_text_pos_beg,between_text_pos_end=who_re2.match(question).span(1)\n",
    "                outside_token_pronidx_set=set()\n",
    "                outside_token_prontext_set=set()\n",
    "                for token in nlp_infor:\n",
    "                    if token.tag_ in self.pron_tag_set:\n",
    "                        pronidx_beg,pronidx_end=token.idx,token.idx+len(token.text)\n",
    "                        if pronidx_end<=between_text_pos_end:\n",
    "                            token_pronidx_infor=f\"{token.idx}_{token.idx+len(token.text)}\"\n",
    "                            outside_token_pronidx_set.add(token_pronidx_infor)\n",
    "                            outside_token_prontext_set.add(question[token.idx:token.idx+len(token.text)].lower())\n",
    "\n",
    "                select_ner_idx=[]\n",
    "                for ent_element in nlp_infor.ents:\n",
    "                    ent_label=ent_element.label_\n",
    "                    if ent_label in select_ner_set:\n",
    "                        if between_text_pos_beg<=ent_element[0].idx<between_text_pos_end:\n",
    "                            select_ner_idx.append(ent_element[0].idx)\n",
    "\n",
    "                add_question=True\n",
    "                for pronidx_tuple in outside_token_pronidx_set:\n",
    "                    pronidx_beg,pronidx_end=list(map(int, pronidx_tuple.split(\"_\")))\n",
    "                    prontext=question[pronidx_beg:pronidx_end].lower()\n",
    "                    if prontext in self.myyou_words_set:\n",
    "                        add_question=False\n",
    "                    if not token_text_set.intersection(who_words_set):\n",
    "                        if prontext in heshethey_words_set:\n",
    "                            add_question=False\n",
    "\n",
    "                if add_question:\n",
    "                    self.good_common_question.add(question)\n",
    "                else:\n",
    "                    self.bad_common_question.add(question)\n",
    "\n",
    "        self.common_candidate_question=self.common_candidate_question-self.good_common_question-self.bad_common_question\n",
    "    \n",
    "    def common_question_processing_step5(self):\n",
    "        for question in self.common_candidate_question:\n",
    "            nlp_infor=self.pron_que_nlp_dict[question]\n",
    "            token_pronidx_set=set()\n",
    "            token_prontext_set=set()\n",
    "            for token in nlp_infor:\n",
    "                if token.tag_ in self.pron_tag_set:\n",
    "                    token_pronidx_infor=f\"{token.idx}_{token.idx+len(token.text)}\"\n",
    "                    token_pronidx_set.add(token_pronidx_infor)\n",
    "                    token_prontext_set.add(question[token.idx:token.idx+len(token.text)].lower())\n",
    "            for prontext in token_prontext_set:\n",
    "                if prontext in self.myyou_words_set:\n",
    "                    self.bad_common_question.add(question)\n",
    "\n",
    "        self.common_candidate_question=self.common_candidate_question-self.good_common_question-self.bad_common_question\n",
    "\n",
    "    def common_question_processing_step6(self):\n",
    "        heshe_words_set=set(['he', 'she', 'they', 'her', 'him', 'them', 'himself',\\\n",
    "                     'hers', 'theirs', 'his', 'their', 'herself', 'themselves'])\n",
    "        select_ner_set=set([\"PERSON\",\"NORP\",\"FAC\",\"ORG\",\"GPE\"])\n",
    "\n",
    "        for question in self.common_candidate_question:\n",
    "            nlp_infor=self.pron_que_nlp_dict[question]\n",
    "            select_ner_idx=[]\n",
    "            for ent_element in nlp_infor.ents:\n",
    "                ent_label=ent_element.label_\n",
    "                if ent_label in select_ner_set:\n",
    "                    select_ner_idx.append(ent_element[0].idx)\n",
    "\n",
    "            myyou_tag=False\n",
    "            heshetoken_pronbegidx_set=set()\n",
    "            heshetoken_prontext_list=[]\n",
    "            for token in nlp_infor:\n",
    "                if token.tag_ in self.pron_tag_set:\n",
    "                    if question[token.idx:token.idx+len(token.text)].lower() in heshe_words_set:\n",
    "                        heshetoken_pronbegidx_set.add(token.idx)\n",
    "                        heshetoken_prontext_list.append(question[token.idx:token.idx+len(token.text)].lower())\n",
    "                    if question[token.idx:token.idx+len(token.text)] in self.myyou_words_set:\n",
    "                        myyou_tag=True\n",
    "\n",
    "            add_question=False\n",
    "            for neridx in select_ner_idx:\n",
    "                for pronbeg in heshetoken_pronbegidx_set:\n",
    "                    if neridx<pronbeg:\n",
    "                        add_question=True\n",
    "                        break\n",
    "            if add_question and (not myyou_tag):\n",
    "                self.good_common_question.add(question)\n",
    "\n",
    "        self.common_candidate_question=self.common_candidate_question-self.good_common_question-self.bad_common_question\n",
    "    \n",
    "    def common_question_processing_step7(self):\n",
    "        for question in self.common_candidate_question:\n",
    "            nlp_infor=self.pron_que_nlp_dict[question]\n",
    "            token_text_set=set([token.text.lower() for token in nlp_infor])\n",
    "            token_pronidx_set=set()\n",
    "            token_prontext_set=set()\n",
    "            for token in nlp_infor:\n",
    "                if token.tag_ in self.pron_tag_set:\n",
    "                    token_pronidx_infor=f\"{token.idx}_{token.idx+len(token.text)}\"\n",
    "                    token_pronidx_set.add(token_pronidx_infor)\n",
    "                    token_prontext_set.add(question[token.idx:token.idx+len(token.text)].lower())\n",
    "            if len(token_prontext_set)==1:\n",
    "                if \"it\" in token_prontext_set:\n",
    "                    self.good_common_question.add(question)\n",
    "        self.common_candidate_question=self.common_candidate_question-self.good_common_question-self.bad_common_question\n",
    "        self.bad_common_question.update(self.common_candidate_question)\n",
    "        self.common_candidate_question = set()\n",
    "        \n",
    "    def pronque_nocoref_processing(self):\n",
    "        self.nocoref_candidate_question = self.pronque_nocoref_set\n",
    "        self.good_nocoref_question = set()\n",
    "        self.bad_nocoref_question = set()        \n",
    "        self.nocoref_question_processing_step1()\n",
    "        self.nocoref_question_processing_step2()\n",
    "        self.nocoref_question_processing_step3()\n",
    "        self.nocoref_question_processing_step4()\n",
    "        self.nocoref_question_processing_step5()\n",
    "        self.nocoref_question_processing_step6()\n",
    "\n",
    "    def nocoref_question_processing_step1(self):\n",
    "        who_re = re.compile(r\"(Who|Whose|Whom) (.*?)(say|says|saying|said)(.*)\")\n",
    "\n",
    "        for question in self.nocoref_candidate_question:\n",
    "            re_result=who_re.match(question)\n",
    "            if re_result:\n",
    "                between_text=who_re.search(question).group(2)\n",
    "                if len(between_text.strip())==0:\n",
    "                    self.good_nocoref_question.add(question)\n",
    "                else:\n",
    "                    between_text_pos_beg,between_text_pos_end=who_re.match(question).span(2)\n",
    "                    nlp_infor=self.pron_que_nlp_dict[question]\n",
    "                    myyoutoken_pronidx_set=set()\n",
    "                    myyoutoken_prontext_list=[]\n",
    "                    for token in nlp_infor:\n",
    "                        if token.tag_ in self.pron_tag_set:\n",
    "                            if question[token.idx:token.idx+len(token.text)] in self.myyou_words_set:\n",
    "                                token_pronidx_infor=f\"{token.idx}_{token.idx+len(token.text)}\"\n",
    "                                myyoutoken_pronidx_set.add(token_pronidx_infor)\n",
    "                                myyoutoken_prontext_list.append(question[token.idx:token.idx+len(token.text)].lower())\n",
    "                    add_question=True\n",
    "                    for pronidx_tuple in myyoutoken_pronidx_set:\n",
    "                        pronidx_beg,pronidx_end=list(map(int, pronidx_tuple.split(\"_\")))\n",
    "                        if between_text_pos_beg<=pronidx_beg<=pronidx_end<=between_text_pos_end:\n",
    "                            add_question=False\n",
    "                    if add_question:\n",
    "                        self.good_nocoref_question.add(question)\n",
    "\n",
    "        self.nocoref_candidate_question=self.nocoref_candidate_question-self.good_nocoref_question-self.bad_nocoref_question\n",
    "\n",
    "    def nocoref_question_processing_step2(self):\n",
    "        who_words_set = set([\"who\",\"whose\",\"whom\"])\n",
    "        he_words_set=set(['he', 'him', 'himself', 'his'])\n",
    "        she_words_set=set(['she', 'her','hers', 'herself'])\n",
    "        they_words_set=set(['they','them','theirs','their','themselves'])\n",
    "\n",
    "        for question in self.nocoref_candidate_question:\n",
    "            nlp_infor=self.pron_que_nlp_dict[question]\n",
    "            token_text_set=set([token.text.lower() for token in nlp_infor])\n",
    "\n",
    "            token_pronidx_set=set()\n",
    "            token_prontext_set=set()\n",
    "            for token in nlp_infor:\n",
    "                if token.tag_ in self.pron_tag_set:\n",
    "                    token_pronidx_infor=f\"{token.idx}_{token.idx+len(token.text)}\"\n",
    "                    token_pronidx_set.add(token_pronidx_infor)\n",
    "                    token_prontext_set.add(question[token.idx:token.idx+len(token.text)].lower())\n",
    "\n",
    "            if token_text_set.intersection(who_words_set):\n",
    "                if token_prontext_set.intersection(self.myyou_words_set):\n",
    "                    continue\n",
    "                else:\n",
    "                    he_in_tag=False\n",
    "                    she_in_tag=False\n",
    "                    they_in_tag=False\n",
    "                    if token_prontext_set.intersection(he_words_set):\n",
    "                        he_in_tag=True\n",
    "                    if token_prontext_set.intersection(she_words_set):\n",
    "                        she_in_tag=True\n",
    "                    if token_prontext_set.intersection(they_words_set):\n",
    "                        they_in_tag=True\n",
    "                    if int(he_in_tag)+int(she_in_tag)+int(they_in_tag)>1:\n",
    "                        self.bad_nocoref_question.add(question)\n",
    "                        continue\n",
    "                    self.good_nocoref_question.add(question)\n",
    "\n",
    "        self.nocoref_candidate_question=self.nocoref_candidate_question-self.good_nocoref_question-self.bad_nocoref_question\n",
    "        \n",
    "    def nocoref_question_processing_step3(self):\n",
    "        who_re2 = re.compile(r\"(.+?)(\\\"(.+)\\\"|''(.+)'')(.*)\")\n",
    "        who_words_set = set([\"who\",\"whose\",\"whom\"])\n",
    "        heshethey_words_set=set(['he', 'she', 'they', 'her', 'him', 'them', 'himself',\\\n",
    "                             'hers', 'theirs', 'his', 'their', 'herself', 'themselves'])\n",
    "        select_ner_set=set([\"PERSON\",\"NORP\",\"FAC\",\"ORG\",\"GPE\"])\n",
    "\n",
    "        for question in self.nocoref_candidate_question:\n",
    "            re_result=who_re2.match(question)\n",
    "            if re_result:\n",
    "                nlp_infor=self.pron_que_nlp_dict[question]\n",
    "                token_text_set=set([token.text.lower() for token in nlp_infor])\n",
    "                between_text_pos_beg,between_text_pos_end=who_re2.match(question).span(1)\n",
    "                outside_token_pronidx_set=set()\n",
    "                outside_token_prontext_set=set()\n",
    "                for token in nlp_infor:\n",
    "                    if token.tag_ in self.pron_tag_set:\n",
    "                        pronidx_beg,pronidx_end=token.idx,token.idx+len(token.text)\n",
    "                        if pronidx_end<=between_text_pos_end:\n",
    "                            token_pronidx_infor=f\"{token.idx}_{token.idx+len(token.text)}\"\n",
    "                            outside_token_pronidx_set.add(token_pronidx_infor)\n",
    "                            outside_token_prontext_set.add(question[token.idx:token.idx+len(token.text)].lower())\n",
    "\n",
    "                select_ner_idx=[]\n",
    "                for ent_element in nlp_infor.ents:\n",
    "                    ent_label=ent_element.label_\n",
    "                    if ent_label in select_ner_set:\n",
    "                        if between_text_pos_beg<=ent_element[0].idx<between_text_pos_end:\n",
    "                            select_ner_idx.append(ent_element[0].idx)\n",
    "\n",
    "                add_question=True\n",
    "                for pronidx_tuple in outside_token_pronidx_set:\n",
    "                    pronidx_beg,pronidx_end=list(map(int, pronidx_tuple.split(\"_\")))\n",
    "                    prontext=question[pronidx_beg:pronidx_end].lower()\n",
    "                    if prontext in self.myyou_words_set:\n",
    "                        add_question=False\n",
    "                    if not token_text_set.intersection(who_words_set):\n",
    "                        if prontext in heshethey_words_set:\n",
    "                            add_question=False\n",
    "\n",
    "                if add_question:\n",
    "                    self.good_nocoref_question.add(question)\n",
    "                else:\n",
    "                    self.bad_nocoref_question.add(question)\n",
    "\n",
    "        self.nocoref_candidate_question=self.nocoref_candidate_question-self.good_nocoref_question-self.bad_nocoref_question\n",
    "        \n",
    "    def nocoref_question_processing_step4(self):\n",
    "\n",
    "        for question in self.nocoref_candidate_question:\n",
    "            nlp_infor=self.pron_que_nlp_dict[question]\n",
    "            token_pronidx_set=set()\n",
    "            token_prontext_set=set()\n",
    "            for token in nlp_infor:\n",
    "                if token.tag_ in self.pron_tag_set:\n",
    "                    token_pronidx_infor=f\"{token.idx}_{token.idx+len(token.text)}\"\n",
    "                    token_pronidx_set.add(token_pronidx_infor)\n",
    "                    token_prontext_set.add(question[token.idx:token.idx+len(token.text)].lower())\n",
    "            for prontext in token_prontext_set:\n",
    "                if prontext in self.myyou_words_set:\n",
    "                    self.bad_nocoref_question.add(question)\n",
    "\n",
    "        self.nocoref_candidate_question=self.nocoref_candidate_question-self.good_nocoref_question-self.bad_nocoref_question\n",
    "        \n",
    "    def nocoref_question_processing_step5(self):\n",
    "        heshe_words_set=set(['he', 'she', 'they', 'her', 'him', 'them', 'himself',\\\n",
    "                     'hers', 'theirs', 'his', 'their', 'herself', 'themselves'])\n",
    "        select_ner_set=set([\"PERSON\",\"NORP\",\"FAC\",\"ORG\",\"GPE\"])\n",
    "\n",
    "        for question in self.nocoref_candidate_question:\n",
    "            nlp_infor=self.pron_que_nlp_dict[question]\n",
    "            select_ner_idx=[]\n",
    "            for ent_element in nlp_infor.ents:\n",
    "                ent_label=ent_element.label_\n",
    "                if ent_label in select_ner_set:\n",
    "                    select_ner_idx.append(ent_element[0].idx)\n",
    "\n",
    "            myyou_tag=False\n",
    "            heshetoken_pronbegidx_set=set()\n",
    "            heshetoken_prontext_list=[]\n",
    "            for token in nlp_infor:\n",
    "                if token.tag_ in self.pron_tag_set:\n",
    "                    if question[token.idx:token.idx+len(token.text)].lower() in heshe_words_set:\n",
    "                        heshetoken_pronbegidx_set.add(token.idx)\n",
    "                        heshetoken_prontext_list.append(question[token.idx:token.idx+len(token.text)].lower())\n",
    "                    if question[token.idx:token.idx+len(token.text)] in self.myyou_words_set:\n",
    "                        myyou_tag=True\n",
    "\n",
    "            add_question=False\n",
    "            for neridx in select_ner_idx:\n",
    "                for pronbeg in heshetoken_pronbegidx_set:\n",
    "                    if neridx<pronbeg:\n",
    "                        add_question=True\n",
    "                        break\n",
    "            if add_question and (not myyou_tag):\n",
    "                self.good_nocoref_question.add(question)\n",
    "\n",
    "        self.nocoref_candidate_question=self.nocoref_candidate_question-self.good_nocoref_question-self.bad_nocoref_question\n",
    "        \n",
    "    def nocoref_question_processing_step6(self):\n",
    "        for question in self.nocoref_candidate_question:\n",
    "            nlp_infor=self.pron_que_nlp_dict[question]\n",
    "            token_text_set=set([token.text.lower() for token in nlp_infor])\n",
    "\n",
    "            token_pronidx_set=set()\n",
    "            token_prontext_set=set()\n",
    "            for token in nlp_infor:\n",
    "                if token.tag_ in self.pron_tag_set:\n",
    "                    token_pronidx_infor=f\"{token.idx}_{token.idx+len(token.text)}\"\n",
    "                    token_pronidx_set.add(token_pronidx_infor)\n",
    "                    token_prontext_set.add(question[token.idx:token.idx+len(token.text)].lower())\n",
    "            if len(token_prontext_set)==1:\n",
    "                if \"it\" in token_prontext_set:\n",
    "                    self.good_nocoref_question.add(question)\n",
    "        self.nocoref_candidate_question=self.nocoref_candidate_question-self.good_nocoref_question-self.bad_nocoref_question\n",
    "        self.bad_nocoref_question.update(self.nocoref_candidate_question)\n",
    "        self.nocoref_candidate_question = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TempInfor_Transformation_Processing:\n",
    "    def __init__(self, results_df, paraid_pub_dict, trans_type):\n",
    "        self.results_df = results_df\n",
    "        self.paraid_pub_dict = paraid_pub_dict\n",
    "        self.trans_type = trans_type\n",
    "    \n",
    "    def __call__(self):\n",
    "        self.temp_trans()\n",
    "        return self.results_df\n",
    "        \n",
    "    def temp_trans(self):\n",
    "        for rowidx, row in self.results_df.iterrows():\n",
    "            org_question = row[\"org_question\"]\n",
    "            org_answer = row[\"org_answer\"]\n",
    "            para_id = row[\"para_id\"]\n",
    "            pub = paraid_pub_dict[para_id]\n",
    "            if self.trans_type==\"question\":\n",
    "                org_infor = org_question\n",
    "            if self.trans_type==\"answer\":\n",
    "                org_infor = org_answer\n",
    "            \n",
    "            ents_list=nlp(org_infor).ents\n",
    "            ents_label_list=[ent.label_ for ent in ents_list]\n",
    "            if \"DATE\" in ents_label_list:\n",
    "                heideltime_parser.set_document_time(pub)\n",
    "                temp_results = heideltime_parser.parse(org_infor)\n",
    "                temp_results = temp_results[temp_results.find(\"\\n<TimeML>\\n\")+len(\"\\n<TimeML>\\n\"):].replace(\"\\n</TimeML>\\n\\n\",\"\")\n",
    "                while re.search(temp_extract_re,temp_results):\n",
    "                    temp_beg, temp_end = re.search(temp_trans_re,temp_results).span()\n",
    "                    temp_infor = temp_results[temp_beg:temp_end]\n",
    "                    temp_value = re.search(temp_extract_re,temp_infor).group(1).strip('\"')\n",
    "                    temp_text = re.search(temp_extract_re,temp_infor).group(2)\n",
    "                    trans_flag = False\n",
    "                    if temp_text!=temp_value:\n",
    "                        if re.fullmatch(temp_day_form,temp_value):\n",
    "                            year_v,month_v,day_v=temp_value.split(\"-\")\n",
    "                            transformed_temp_value=f\"{idx2month[month_v]} {day_v}, {year_v}\"\n",
    "                            trans_flag = True\n",
    "                        elif re.fullmatch(temp_month_form,temp_value):\n",
    "                            year_v,month_v=temp_value.split(\"-\")\n",
    "                            transformed_temp_value=f\"{idx2month[month_v]}, {year_v}\"\n",
    "                            trans_flag = True\n",
    "                        elif re.fullmatch(temp_year_form,temp_value):\n",
    "                            temp_type=\"year\"\n",
    "                            year_v=temp_value\n",
    "                            transformed_temp_value=year_v\n",
    "                            trans_flag = True\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        break\n",
    "                    if trans_flag:\n",
    "                        trans_text = temp_results[:temp_beg]+transformed_temp_value+temp_results[temp_end:]\n",
    "                        temp_results = trans_text\n",
    "                        if self.trans_type==\"question\":\n",
    "                            self.results_df.iloc[rowidx][\"org_question\"]=trans_text\n",
    "                            self.results_df.iloc[rowidx][\"trans_que\"]=\"1\"\n",
    "                        if self.trans_type==\"answer\":\n",
    "                            self.results_df.iloc[rowidx][\"org_answer\"]=trans_text\n",
    "                            self.results_df.iloc[rowidx][\"trans_ans\"]=\"1\"\n",
    "                        self.results_df.iloc[rowidx][f\"org_{self.trans_type}\"]=trans_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "example=pickle.load(open(\"data/examples.pickle\", \"rb\"))\n",
    "paraid_pub_dict = dict()\n",
    "for r in example:\n",
    "    paraid_pub_dict[r[0]] = r[1][:4]+\"-\"+r[1][4:6]+\"-\"+r[1][6:8]\n",
    "    \n",
    "raw_results_df=feather.read_feather(\"data/raw_results_After_2ndModule.feather\")\n",
    "print(len(raw_results_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ent_num,max_ent_num = 1,7\n",
    "min_token_num,max_token_num = 8,30\n",
    "ST_Processing = Syntactic_Temporal_Processing(min_ent_num, max_ent_num,\\\n",
    "                                              min_token_num, max_token_num, paraid_pub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "basic_filtered_results_df = ST_Processing(raw_results_df)\n",
    "print(len(basic_filtered_results_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_filtered_results_df.to_feather(\"data/raw_results_After_3rdModule.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic_filtered_results_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic_filtered_results_df[basic_filtered_results_df[\"trans_que\"]==\"1\"][[\"org_question\",\"trans_question\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic_filtered_results_df[basic_filtered_results_df[\"trans_ans\"]==\"1\"][[\"org_answer\",\"trans_answer\"]].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
